{
  "model": "Qwen/Qwen3-30B-A3B",
  "timestamp": "2025-09-05 18:57:43",
  "total_benchmarks": 5,
  "successful_benchmarks": 5,
  "failed_benchmarks": 0,
  "results": [
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=20, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=64, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=1.0, max_concurrency=1, output_file='qwen3_benchmark_results/ttft_ttft_baseline.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=20, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=64, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=1.0, max_concurrency=1, output_file='qwen3_benchmark_results/ttft_ttft_baseline.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 5201\n#Output tokens: 541\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    1.0       \nMax request concurrency:                 1         \nSuccessful requests:                     20        \nBenchmark duration (s):                  12.66     \nTotal input tokens:                      5201      \nTotal generated tokens:                  542       \nTotal generated tokens (retokenized):    542       \nRequest throughput (req/s):              1.58      \nInput token throughput (tok/s):          410.89    \nOutput token throughput (tok/s):         42.82     \nTotal token throughput (tok/s):          453.71    \nConcurrency:                             0.36      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   229.59    \nMedian E2E Latency (ms):                 168.72    \n---------------Time to First Token----------------\nMean TTFT (ms):                          53.86     \nMedian TTFT (ms):                        54.66     \nP99 TTFT (ms):                           69.95     \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           6.73      \nMedian ITL (ms):                         6.76      \nP95 ITL (ms):                            6.94      \nP99 ITL (ms):                            7.06      \nMax ITL (ms):                            7.24      \n==================================================\n",
      "error": null,
      "config": {
        "name": "ttft_baseline",
        "dataset": "random",
        "num_prompts": 20,
        "request_rate": 1.0,
        "max_concurrency": 1,
        "random_input_len": 512,
        "random_output_len": 64,
        "description": "Quick TTFT baseline"
      }
    },
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=30, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=5.0, max_concurrency=4, output_file='qwen3_benchmark_results/ttft_ttft_concurrent.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=30, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=5.0, max_concurrency=4, output_file='qwen3_benchmark_results/ttft_ttft_concurrent.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 15964\n#Output tokens: 1734\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    5.0       \nMax request concurrency:                 4         \nSuccessful requests:                     30        \nBenchmark duration (s):                  6.72      \nTotal input tokens:                      15964     \nTotal generated tokens:                  1734      \nTotal generated tokens (retokenized):    1734      \nRequest throughput (req/s):              4.46      \nInput token throughput (tok/s):          2375.24   \nOutput token throughput (tok/s):         258.00    \nTotal token throughput (tok/s):          2633.23   \nConcurrency:                             3.64      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   815.64    \nMedian E2E Latency (ms):                 876.48    \n---------------Time to First Token----------------\nMean TTFT (ms):                          66.98     \nMedian TTFT (ms):                        68.63     \nP99 TTFT (ms):                           111.96    \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           13.18     \nMedian ITL (ms):                         11.50     \nP95 ITL (ms):                            33.91     \nP99 ITL (ms):                            40.27     \nMax ITL (ms):                            110.42    \n==================================================\n",
      "error": null,
      "config": {
        "name": "ttft_concurrent",
        "dataset": "random",
        "num_prompts": 30,
        "request_rate": 5.0,
        "max_concurrency": 4,
        "random_input_len": 1024,
        "random_output_len": 128,
        "description": "TTFT with some concurrency"
      }
    },
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=inf, max_concurrency=16, output_file='qwen3_benchmark_results/throughput_throughput_burst.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=inf, max_concurrency=16, output_file='qwen3_benchmark_results/throughput_throughput_burst.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 14155\n#Output tokens: 2888\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    inf       \nMax request concurrency:                 16        \nSuccessful requests:                     50        \nBenchmark duration (s):                  5.72      \nTotal input tokens:                      14155     \nTotal generated tokens:                  2888      \nTotal generated tokens (retokenized):    2888      \nRequest throughput (req/s):              8.73      \nInput token throughput (tok/s):          2472.56   \nOutput token throughput (tok/s):         504.47    \nTotal token throughput (tok/s):          2977.03   \nConcurrency:                             13.88     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   1589.71   \nMedian E2E Latency (ms):                 1444.05   \n---------------Time to First Token----------------\nMean TTFT (ms):                          80.86     \nMedian TTFT (ms):                        86.62     \nP99 TTFT (ms):                           125.51    \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           26.58     \nMedian ITL (ms):                         22.63     \nP95 ITL (ms):                            55.96     \nP99 ITL (ms):                            62.86     \nMax ITL (ms):                            106.28    \n==================================================\n",
      "error": null,
      "config": {
        "name": "throughput_burst",
        "dataset": "random",
        "num_prompts": 50,
        "request_rate": Infinity,
        "max_concurrency": 16,
        "random_input_len": 512,
        "random_output_len": 128,
        "description": "Quick burst throughput test"
      }
    },
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=40, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=256, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=10.0, max_concurrency=8, output_file='qwen3_benchmark_results/throughput_throughput_sustained.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=40, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=256, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=10.0, max_concurrency=8, output_file='qwen3_benchmark_results/throughput_throughput_sustained.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 21999\n#Output tokens: 4572\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    10.0      \nMax request concurrency:                 8         \nSuccessful requests:                     40        \nBenchmark duration (s):                  11.08     \nTotal input tokens:                      21999     \nTotal generated tokens:                  4572      \nTotal generated tokens (retokenized):    4572      \nRequest throughput (req/s):              3.61      \nInput token throughput (tok/s):          1985.63   \nOutput token throughput (tok/s):         412.67    \nTotal token throughput (tok/s):          2398.30   \nConcurrency:                             6.86      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   1901.12   \nMedian E2E Latency (ms):                 1957.57   \n---------------Time to First Token----------------\nMean TTFT (ms):                          55.72     \nMedian TTFT (ms):                        50.86     \nP99 TTFT (ms):                           86.72     \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           16.29     \nMedian ITL (ms):                         15.65     \nP95 ITL (ms):                            28.43     \nP99 ITL (ms):                            39.71     \nMax ITL (ms):                            62.77     \n==================================================\n",
      "error": null,
      "config": {
        "name": "throughput_sustained",
        "dataset": "random",
        "num_prompts": 40,
        "request_rate": 10.0,
        "max_concurrency": 8,
        "random_input_len": 1024,
        "random_output_len": 256,
        "description": "Sustained throughput baseline"
      }
    },
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang-oai-chat', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=20, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=5.0, max_concurrency=4, output_file='qwen3_benchmark_results/chat_chat_api_baseline.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=True, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang-oai-chat', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='Qwen/Qwen3-30B-A3B', tokenizer=None, num_prompts=20, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=5.0, max_concurrency=4, output_file='qwen3_benchmark_results/chat_chat_api_baseline.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=True, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 5201\n#Output tokens: 1251\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai-chat\nTraffic request rate:                    5.0       \nMax request concurrency:                 4         \nSuccessful requests:                     20        \nBenchmark duration (s):                  4.51      \nTotal input tokens:                      5201      \nTotal generated tokens:                  1251      \nTotal generated tokens (retokenized):    1251      \nRequest throughput (req/s):              4.43      \nInput token throughput (tok/s):          1152.20   \nOutput token throughput (tok/s):         277.14    \nTotal token throughput (tok/s):          1429.34   \nConcurrency:                             3.76      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   847.68    \nMedian E2E Latency (ms):                 890.36    \n---------------Time to First Token----------------\nMean TTFT (ms):                          61.58     \nMedian TTFT (ms):                        59.93     \nP99 TTFT (ms):                           85.00     \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           12.77     \nMedian ITL (ms):                         11.24     \nP95 ITL (ms):                            30.11     \nP99 ITL (ms):                            40.98     \nMax ITL (ms):                            52.83     \n==================================================\n",
      "error": null,
      "config": {
        "name": "chat_api_baseline",
        "dataset": "random",
        "num_prompts": 20,
        "request_rate": 5.0,
        "max_concurrency": 4,
        "random_input_len": 512,
        "random_output_len": 128,
        "description": "Quick chat API test"
      }
    }
  ]
}