{
  "model": "Qwen/Qwen3-30B-A3B",
  "timestamp": "2025-09-05 19:57:31",
  "total_benchmarks": 5,
  "successful_benchmarks": 5,
  "failed_benchmarks": 0,
  "results": [
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=20, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=64, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=1.0, max_concurrency=1, output_file='qwen_qwen3_8b_sglang_benchmark_results/ttft_ttft_baseline.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=20, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=64, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=1.0, max_concurrency=1, output_file='qwen_qwen3_8b_sglang_benchmark_results/ttft_ttft_baseline.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 5201\n#Output tokens: 541\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    1.0       \nMax request concurrency:                 1         \nSuccessful requests:                     20        \nBenchmark duration (s):                  12.65     \nTotal input tokens:                      5201      \nTotal generated tokens:                  542       \nTotal generated tokens (retokenized):    542       \nRequest throughput (req/s):              1.58      \nInput token throughput (tok/s):          411.00    \nOutput token throughput (tok/s):         42.83     \nTotal token throughput (tok/s):          453.83    \nConcurrency:                             0.48      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   300.82    \nMedian E2E Latency (ms):                 218.59    \n---------------Time to First Token----------------\nMean TTFT (ms):                          33.74     \nMedian TTFT (ms):                        34.06     \nP99 TTFT (ms):                           42.99     \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           10.23     \nMedian ITL (ms):                         10.25     \nP95 ITL (ms):                            10.38     \nP99 ITL (ms):                            10.50     \nMax ITL (ms):                            13.07     \n==================================================\n",
      "error": null,
      "config": {
        "name": "ttft_baseline",
        "dataset": "random",
        "num_prompts": 20,
        "request_rate": 1.0,
        "max_concurrency": 1,
        "random_input_len": 512,
        "random_output_len": 64,
        "description": "Quick TTFT baseline"
      }
    },
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=30, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=5.0, max_concurrency=4, output_file='qwen_qwen3_8b_sglang_benchmark_results/ttft_ttft_concurrent.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=30, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=5.0, max_concurrency=4, output_file='qwen_qwen3_8b_sglang_benchmark_results/ttft_ttft_concurrent.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 15964\n#Output tokens: 1734\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    5.0       \nMax request concurrency:                 4         \nSuccessful requests:                     30        \nBenchmark duration (s):                  6.61      \nTotal input tokens:                      15964     \nTotal generated tokens:                  1734      \nTotal generated tokens (retokenized):    1734      \nRequest throughput (req/s):              4.54      \nInput token throughput (tok/s):          2413.85   \nOutput token throughput (tok/s):         262.19    \nTotal token throughput (tok/s):          2676.04   \nConcurrency:                             3.13      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   689.97    \nMedian E2E Latency (ms):                 679.23    \n---------------Time to First Token----------------\nMean TTFT (ms):                          42.62     \nMedian TTFT (ms):                        41.04     \nP99 TTFT (ms):                           87.75     \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           11.40     \nMedian ITL (ms):                         10.61     \nP95 ITL (ms):                            18.19     \nP99 ITL (ms):                            26.78     \nMax ITL (ms):                            96.62     \n==================================================\n",
      "error": null,
      "config": {
        "name": "ttft_concurrent",
        "dataset": "random",
        "num_prompts": 30,
        "request_rate": 5.0,
        "max_concurrency": 4,
        "random_input_len": 1024,
        "random_output_len": 128,
        "description": "TTFT with some concurrency"
      }
    },
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=inf, max_concurrency=16, output_file='qwen_qwen3_8b_sglang_benchmark_results/throughput_throughput_burst.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=50, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=inf, max_concurrency=16, output_file='qwen_qwen3_8b_sglang_benchmark_results/throughput_throughput_burst.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 14155\n#Output tokens: 2888\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    inf       \nMax request concurrency:                 16        \nSuccessful requests:                     50        \nBenchmark duration (s):                  3.29      \nTotal input tokens:                      14155     \nTotal generated tokens:                  2888      \nTotal generated tokens (retokenized):    2888      \nRequest throughput (req/s):              15.21     \nInput token throughput (tok/s):          4304.89   \nOutput token throughput (tok/s):         878.31    \nTotal token throughput (tok/s):          5183.20   \nConcurrency:                             12.50     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   822.10    \nMedian E2E Latency (ms):                 744.57    \n---------------Time to First Token----------------\nMean TTFT (ms):                          40.14     \nMedian TTFT (ms):                        41.91     \nP99 TTFT (ms):                           56.05     \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           13.78     \nMedian ITL (ms):                         11.28     \nP95 ITL (ms):                            27.62     \nP99 ITL (ms):                            31.81     \nMax ITL (ms):                            54.36     \n==================================================\n",
      "error": null,
      "config": {
        "name": "throughput_burst",
        "dataset": "random",
        "num_prompts": 50,
        "request_rate": Infinity,
        "max_concurrency": 16,
        "random_input_len": 512,
        "random_output_len": 128,
        "description": "Quick burst throughput test"
      }
    },
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=40, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=256, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=10.0, max_concurrency=8, output_file='qwen_qwen3_8b_sglang_benchmark_results/throughput_throughput_sustained.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=40, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=256, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=10.0, max_concurrency=8, output_file='qwen_qwen3_8b_sglang_benchmark_results/throughput_throughput_sustained.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 21999\n#Output tokens: 4572\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    10.0      \nMax request concurrency:                 8         \nSuccessful requests:                     40        \nBenchmark duration (s):                  8.16      \nTotal input tokens:                      21999     \nTotal generated tokens:                  4572      \nTotal generated tokens (retokenized):    4572      \nRequest throughput (req/s):              4.90      \nInput token throughput (tok/s):          2697.43   \nOutput token throughput (tok/s):         560.60    \nTotal token throughput (tok/s):          3258.03   \nConcurrency:                             6.62      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   1350.42   \nMedian E2E Latency (ms):                 1402.45   \n---------------Time to First Token----------------\nMean TTFT (ms):                          34.74     \nMedian TTFT (ms):                        32.23     \nP99 TTFT (ms):                           59.86     \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           11.61     \nMedian ITL (ms):                         10.95     \nP95 ITL (ms):                            15.25     \nP99 ITL (ms):                            23.00     \nMax ITL (ms):                            49.51     \n==================================================\n",
      "error": null,
      "config": {
        "name": "throughput_sustained",
        "dataset": "random",
        "num_prompts": 40,
        "request_rate": 10.0,
        "max_concurrency": 8,
        "random_input_len": 1024,
        "random_output_len": 256,
        "description": "Sustained throughput baseline"
      }
    },
    {
      "success": true,
      "output": "benchmark_args=Namespace(backend='sglang-oai-chat', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=20, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=5.0, max_concurrency=4, output_file='qwen_qwen3_8b_sglang_benchmark_results/chat_chat_api_baseline.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=True, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang-oai-chat', base_url=None, host='127.0.0.1', port=30000, dataset_name='random', dataset_path='', model='qwen/qwen3-8b', tokenizer=None, num_prompts=20, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_image_num_images=1, random_image_resolution='1080p', request_rate=5.0, max_concurrency=4, output_file='qwen_qwen3_8b_sglang_benchmark_results/chat_chat_api_baseline.jsonl', output_details=True, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=True, extra_request_body=None, apply_chat_template=True, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 5201\n#Output tokens: 1251\nStarting warmup with 1 sequences...\nWarmup completed with 1 sequences. Starting main benchmark run...\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai-chat\nTraffic request rate:                    5.0       \nMax request concurrency:                 4         \nSuccessful requests:                     20        \nBenchmark duration (s):                  3.96      \nTotal input tokens:                      5201      \nTotal generated tokens:                  1251      \nTotal generated tokens (retokenized):    1251      \nRequest throughput (req/s):              5.04      \nInput token throughput (tok/s):          1311.89   \nOutput token throughput (tok/s):         315.55    \nTotal token throughput (tok/s):          1627.43   \nConcurrency:                             3.67      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   727.61    \nMedian E2E Latency (ms):                 733.97    \n---------------Time to First Token----------------\nMean TTFT (ms):                          34.96     \nMedian TTFT (ms):                        34.03     \nP99 TTFT (ms):                           44.15     \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           11.25     \nMedian ITL (ms):                         10.54     \nP95 ITL (ms):                            17.79     \nP99 ITL (ms):                            25.59     \nMax ITL (ms):                            29.90     \n==================================================\n",
      "error": null,
      "config": {
        "name": "chat_api_baseline",
        "dataset": "random",
        "num_prompts": 20,
        "request_rate": 5.0,
        "max_concurrency": 4,
        "random_input_len": 512,
        "random_output_len": 128,
        "description": "Quick chat API test"
      }
    }
  ]
}